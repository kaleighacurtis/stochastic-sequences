{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating all the files used to train the model and the model itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import json\n",
    "from ax.service.managed_loop import optimize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPRegressor(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size=(100, ), activation=nn.ReLU()):\n",
    "        torch.manual_seed(0)  # control random effects\n",
    "\n",
    "        super(MLPRegressor, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        prev_layer_size = input_size\n",
    "        for layer_size in hidden_size:\n",
    "            layers.append(nn.Linear(prev_layer_size, layer_size))\n",
    "            layers.append(activation)\n",
    "            prev_layer_size = layer_size\n",
    "\n",
    "        layers.append(nn.Linear(prev_layer_size, output_size))\n",
    "        self.fc_layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "    \n",
    "import tqdm\n",
    "\n",
    "def train_model(model, data, optimizer, n_epochs):\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    x_train, x_val, y_train, y_val = data\n",
    "\n",
    "    # do the training\n",
    "    pbar = tqdm.tqdm(np.arange(n_epochs))\n",
    "    for epoch in pbar:\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        yp = model(x_train)\n",
    "        # Compute Loss\n",
    "        loss = criterion(yp, y_train)\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Update progress bar\n",
    "        pbar.set_postfix_str(f'loss: {loss.item():.3e}')\n",
    "\n",
    "    return model\n",
    "\n",
    "# create a data structure to convert from str input to nn function\n",
    "activation_dict = {'tanh': nn.Tanh(),\n",
    "                   'relu': nn.ReLU(),\n",
    "                   'leaky_relu': nn.LeakyReLU(),\n",
    "                   'sigmoid': nn.Sigmoid(),\n",
    "                   'elu': nn.ELU()}\n",
    "\n",
    "def generate_model_and_optimizer(params):\n",
    "    # create a list of layer sizes from the start, end, and depth\n",
    "    raw_dims = np.linspace(params[\"layer_i\"], params[\"layer_f\"], params[\"num_layers\"])\n",
    "    hidden_size = tuple(raw_dims.round().astype(int))\n",
    "    # choose the activation function\n",
    "    activation = activation_dict[params['activation']]\n",
    "\n",
    "    # initialize the model\n",
    "    model = MLPRegressor(3, 2, hidden_size, activation)\n",
    "\n",
    "    # initialize the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "\n",
    "    # return both objects to calling function\n",
    "    return model, optimizer\n",
    "\n",
    "def mlp_fitness(params):\n",
    "    # the evaluation function cannot accept arguments -- we hard-code them here\n",
    "\n",
    "    data = (x_train, x_val, y_train, y_val)\n",
    "    try:\n",
    "        # train on training set\n",
    "        model, optimizer = generate_model_and_optimizer(params)\n",
    "        train_model(model, data, optimizer, params['n_epochs'])\n",
    "        # evaluate on held-out validation set\n",
    "        y_pred = model(x_val).detach()\n",
    "        rmse = torch.sqrt( torch.mean( (y_pred - y_val)**2 ) ).item()\n",
    "    except:\n",
    "        rmse = 1e12\n",
    "    rmse = min(rmse, 1e2)  # need to choose a threshold RMSE to handle errors\n",
    "    return np.log(rmse)    # log will be better behaved for very small numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting data for the model \n",
    "\n",
    "# Import processed embedding file, nn_input, and nn_output\n",
    "df = pd.read_csv('no_avg_dataset.csv')\n",
    "nia = np.load(\"nn_input.npy\")\n",
    "noa = np.load(\"nn_output.npy\")\n",
    "seq_array = np.loadtxt(\"seq_arr.npy\", dtype = str).reshape(-1,1)\n",
    "\n",
    "# Get training, testing, and validation datasets, split by sequence\n",
    "test_val_splitter = GroupShuffleSplit(n_splits = 1, test_size = 0.6, random_state = 0)\n",
    "\n",
    "test_val_indices, train_indices = next(test_val_splitter.split(nia, noa, groups = seq_array))\n",
    "\n",
    "x_test_val, x_train = nia[test_val_indices], nia[train_indices]\n",
    "y_test_val, y_train = noa[test_val_indices], noa[train_indices]\n",
    "groups_test_val = seq_array[test_val_indices]\n",
    "\n",
    "val_test_splitter = GroupShuffleSplit(n_splits=1, test_size=0.5, random_state=0)\n",
    "val_indices, test_indices = next(val_test_splitter.split(x_test_val, y_test_val, groups=groups_test_val))\n",
    "\n",
    "x_val, x_test = x_test_val[val_indices], x_test_val[test_indices]\n",
    "y_val, y_test = y_test_val[val_indices], y_test_val[test_indices]\n",
    "\n",
    "test_seqs = seq_array[test_indices]\n",
    "val_seqs = seq_array[val_indices]\n",
    "\n",
    "#Turn them into tensors\n",
    "x_train = torch.Tensor(x_train)\n",
    "x_val = torch.Tensor(x_val)\n",
    "y_train = torch.Tensor(y_train)\n",
    "y_val = torch.Tensor(y_val)\n",
    "x_test = torch.Tensor(x_test)\n",
    "y_test = torch.Tensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters, values, experiment, ax_model = optimize(\n",
    "    parameters=[\n",
    "        {\"name\": \"lr\", \"type\": \"range\", \"bounds\": [1e-4, 1e0], \"log_scale\": True},\n",
    "        {\"name\": \"n_epochs\", \"type\": \"range\", \"bounds\": [100, 2000]},\n",
    "        {\"name\": \"activation\", \"type\": \"choice\", \"values\": [\"sigmoid\", \"tanh\", \"relu\", \"leaky_relu\", \"elu\"], \"is_ordered\": False},\n",
    "        {\"name\": \"num_layers\", \"type\": \"range\", \"bounds\": [1, 6]},\n",
    "        {\"name\": \"layer_i\", \"type\": \"range\", \"bounds\": [1,128]},\n",
    "        {\"name\": \"layer_f\", \"type\": \"range\", \"bounds\": [1,64]},\n",
    "    ],\n",
    "    evaluation_function=mlp_fitness,\n",
    "    objective_name='rmse',\n",
    "    minimize=True,\n",
    "    total_trials=25,\n",
    "    random_seed=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving best parameters\n",
    "with open('best_parameters.json', 'w') as f:\n",
    "    json.dump(best_parameters, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1185/1185 [00:06<00:00, 176.92it/s, loss: 8.613e-01]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(\n",
       "  (fc_layers): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=79, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=79, out_features=68, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=68, out_features=56, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=56, out_features=45, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=45, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('best_parameters.json', 'r') as f:\n",
    "    best_parameters = json.load(f)\n",
    "\n",
    "model, optimizer = generate_model_and_optimizer(best_parameters)\n",
    "x_train = torch.Tensor(x_train)\n",
    "x_val = torch.Tensor(x_val)\n",
    "y_train = torch.Tensor(y_train)\n",
    "y_val = torch.Tensor(y_val)\n",
    "x_test = torch.Tensor(x_test)\n",
    "y_test = torch.Tensor(y_test)\n",
    "\n",
    "data = (x_train, x_val, y_train, y_val)\n",
    "train_model(model, data, optimizer, best_parameters['n_epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model \n",
    "torch.save(model.state_dict(), 'embedding_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
