{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to generate all data used for figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from torch import nn\n",
    "import json\n",
    "import csv\n",
    "from alphashape import alphashape\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making no_avg_dataset.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is the processed embedding file. The processing takes the p value and run number from the file name and adds them as columns in the dataframe. It also removes the restart_box entries which are not used to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to process the file\n",
    "def extract_p(filename):\n",
    "    p_junk = filename.split(\"_\")[-1]\n",
    "    p = float(p_junk.replace(\".gsd\", \"\").replace(\"p\", \"\"))\n",
    "    return p\n",
    "\n",
    "def process_df(data):\n",
    "    #Defining lists I use\n",
    "    p_vals = []\n",
    "    run_nos = []\n",
    "\n",
    "    #Removing rows that are just the restart box\n",
    "    for i, seq in enumerate(data['Sequence']):\n",
    "        if len(seq)>21:\n",
    "            data.drop(i, axis = 0, inplace = True)\n",
    "\n",
    "    #Getting p values and run numbers from file names\n",
    "    for file in data[\"Filename\"]:\n",
    "        p = extract_p(file)\n",
    "        p_vals.append(p)\n",
    "\n",
    "        run = float(file.split(\"_\")[7])\n",
    "        run_nos.append(run)\n",
    "    \n",
    "    #Adding p and run number to dataframe\n",
    "    data['p'] = p_vals\n",
    "    data['Run number'] = run_nos\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with the embedding file from UMAP\n",
    "embedding_file = 'embeddings-8.csv'\n",
    "\n",
    "# Load embeddings\n",
    "df = pd.read_csv(embedding_file)\n",
    "\n",
    "# Get processed file\n",
    "processed_df = process_df(df)\n",
    "\n",
    "# Save csv file\n",
    "processed_df.to_csv('no_avg_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making nn_input.npy, nn_output.npy, and nn_inputs_with_seq.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files are the neural network training data. The input file contains the starting Z coordinate for a sequence and a p value, and the output file contains the real coordinate for that sequence at that p value. nn_inputs_wiht_seq.csv is the neural network input array with the corresponding sequence for each row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import processed embedding file \n",
    "df = pd.read_csv('no_avg_dataset.csv')\n",
    "\n",
    "# Get unique sequences and p values\n",
    "seqs = np.unique(df[\"Sequence\"])\n",
    "ps = np.unique(df[\"p\"])\n",
    "\n",
    "# Create empty arrays to store values in later\n",
    "nn_input_array = np.empty((0,3))\n",
    "nn_output_array = np.empty((0,2))\n",
    "seq_array = np.empty((0,1))\n",
    "\n",
    "# Loop through each sequence\n",
    "for seq in seqs:\n",
    "    seq_dataset = df[df[\"Sequence\"] == seq]\n",
    "\n",
    "    # Get the p=0 point\n",
    "    p0_seq_dataset = seq_dataset[seq_dataset[\"p\"] == 0]\n",
    "    p0_z0 = p0_seq_dataset[\"Z0\"]\n",
    "    p0_z1 = p0_seq_dataset[\"Z1\"]\n",
    "\n",
    "    # Get the z coordinate at the other p values \n",
    "    for p in ps:\n",
    "        p_seq_dataset = seq_dataset[seq_dataset[\"p\"] == p]\n",
    "        p_z0 = p_seq_dataset[\"Z0\"]\n",
    "        p_z1 = p_seq_dataset[\"Z1\"]\n",
    "\n",
    "        p_arr = np.full((5,1), p)\n",
    "\n",
    "        # Put data into output arrays\n",
    "        input_row = np.concatenate((np.asarray(p0_z0).reshape(-1,1), np.asarray(p0_z1).reshape(-1,1), p_arr), axis = 1)\n",
    "        output_row = np.concatenate((np.asarray(p_z0).reshape(-1,1), np.asarray(p_z1).reshape(-1,1)), axis = 1)\n",
    "        seq_row = np.asarray(p_seq_dataset[\"Sequence\"]).reshape(-1,1)\n",
    "\n",
    "\n",
    "        nn_input_array = np.vstack((nn_input_array, input_row))\n",
    "        nn_output_array = np.vstack((nn_output_array, output_row))\n",
    "        seq_array = np.vstack((seq_array, seq_row))\n",
    "\n",
    "# Redefine arrays for simplicity\n",
    "nia = nn_input_array\n",
    "noa = nn_output_array\n",
    "\n",
    "# Generate nia with sequence array \n",
    "nia_with_seq = np.concatenate((nia, seq_array), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write arrays to numpy files\n",
    "np.save(\"nn_input.npy\", nia)\n",
    "np.save(\"nn_output.npy\", noa)\n",
    "\n",
    "# Write nia_with_seq to csv file \n",
    "np.savetxt(\"nn_inputs_with_seq.csv\", nia_with_seq, delimiter = ',', fmt = \"%s\") # depending on formatting, fmt might not be required\n",
    "\n",
    "# Save seq array for making the next csv file \n",
    "np.savetxt(\"seq_arr.npy\", seq_array, fmt = \"%s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making error_fp_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import processed embedding file, nn_input, and nn_output\n",
    "df = pd.read_csv('no_avg_dataset.csv')\n",
    "nia = np.load(\"nn_input.npy\")\n",
    "noa = np.load(\"nn_output.npy\")\n",
    "seq_array = np.loadtxt(\"seq_arr.npy\", dtype = str).reshape(-1,1)\n",
    "\n",
    "# Get training, testing, and validation datasets, split by sequence\n",
    "test_val_splitter = GroupShuffleSplit(n_splits = 1, test_size = 0.6, random_state = 0)\n",
    "\n",
    "test_val_indices, train_indices = next(test_val_splitter.split(nia, noa, groups = seq_array))\n",
    "\n",
    "x_test_val, x_train = nia[test_val_indices], nia[train_indices]\n",
    "y_test_val, y_train = noa[test_val_indices], noa[train_indices]\n",
    "groups_test_val = seq_array[test_val_indices]\n",
    "\n",
    "val_test_splitter = GroupShuffleSplit(n_splits=1, test_size=0.5, random_state=0)\n",
    "val_indices, test_indices = next(val_test_splitter.split(x_test_val, y_test_val, groups=groups_test_val))\n",
    "\n",
    "x_val, x_test = x_test_val[val_indices], x_test_val[test_indices]\n",
    "y_val, y_test = y_test_val[val_indices], y_test_val[test_indices]\n",
    "\n",
    "test_seqs = seq_array[test_indices]\n",
    "val_seqs = seq_array[val_indices]\n",
    "\n",
    "#Turn them into tensors\n",
    "x_train = torch.Tensor(x_train)\n",
    "x_val = torch.Tensor(x_val)\n",
    "y_train = torch.Tensor(y_train)\n",
    "y_val = torch.Tensor(y_val)\n",
    "x_test = torch.Tensor(x_test)\n",
    "y_test = torch.Tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model to get the predicted values\n",
    "\n",
    "class MLPRegressor(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size=(100, ), activation=nn.ReLU()):\n",
    "        torch.manual_seed(0)  # control random effects\n",
    "\n",
    "        super(MLPRegressor, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        prev_layer_size = input_size\n",
    "        for layer_size in hidden_size:\n",
    "            layers.append(nn.Linear(prev_layer_size, layer_size))\n",
    "            layers.append(activation)\n",
    "            prev_layer_size = layer_size\n",
    "\n",
    "        layers.append(nn.Linear(prev_layer_size, output_size))\n",
    "        self.fc_layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = x.clone().detach().requires_grad_(True)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "def train_model(model, data, optimizer, n_epochs):\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    x_train, x_val, y_train, y_val = data\n",
    "\n",
    "    # do the training\n",
    "    pbar = tqdm.tqdm(np.arange(n_epochs))\n",
    "    for epoch in pbar:\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        yp = model(x_train)\n",
    "        # Compute Loss\n",
    "        loss = criterion(yp, y_train)\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Update progress bar\n",
    "        pbar.set_postfix_str(f'loss: {loss.item():.3e}')\n",
    "\n",
    "    return model\n",
    "\n",
    "# create a data structure to convert from str input to nn function\n",
    "activation_dict = {'tanh': nn.Tanh(),\n",
    "                   'relu': nn.ReLU(),\n",
    "                   'leaky_relu': nn.LeakyReLU(),\n",
    "                   'sigmoid': nn.Sigmoid(),\n",
    "                   'elu': nn.ELU()}\n",
    "\n",
    "def generate_model_and_optimizer(params):\n",
    "    # create a list of layer sizes from the start, end, and depth\n",
    "    raw_dims = np.linspace(params[\"layer_i\"], params[\"layer_f\"], params[\"num_layers\"])\n",
    "    hidden_size = tuple(raw_dims.round().astype(int))\n",
    "    # choose the activation function\n",
    "    activation = activation_dict[params['activation']]\n",
    "\n",
    "    # initialize the model\n",
    "    model = MLPRegressor(3, 2, hidden_size, activation)\n",
    "\n",
    "    # initialize the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "\n",
    "    # return both objects to calling function\n",
    "    return model, optimizer\n",
    "\n",
    "def mlp_fitness(params):\n",
    "    # the evaluation function cannot accept arguments -- we hard-code them here\n",
    "\n",
    "    data = (x_train, x_val, y_train, y_val)\n",
    "    try:\n",
    "        # train on training set\n",
    "        model, optimizer = generate_model_and_optimizer(params)\n",
    "        train_model(model, data, optimizer, params['n_epochs'])\n",
    "        # evaluate on held-out validation set\n",
    "        y_pred = model(x_val).detach()\n",
    "        rmse = torch.sqrt( torch.mean( (y_pred - y_val)**2 ) ).item()\n",
    "    except:\n",
    "        rmse = 1e12\n",
    "    rmse = min(rmse, 1e2)  # need to choose a threshold RMSE to handle errors\n",
    "    return np.log(rmse)    # log will be better behaved for very small numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1185/1185 [00:05<00:00, 205.50it/s, loss: 8.613e-01]\n"
     ]
    }
   ],
   "source": [
    "# Load model \n",
    "with open('best_parameters_absolute.json', 'r') as f:\n",
    "    best_parameters = json.load(f)\n",
    "\n",
    "model, optimizer = generate_model_and_optimizer(best_parameters)\n",
    "\n",
    "# Get predictedvalues\n",
    "data = (x_train, x_val, y_train, y_val)\n",
    "train_model(model, data, optimizer, best_parameters['n_epochs'])\n",
    "# train rmse\n",
    "y_pred_train = model(x_train).detach()\n",
    "rmse_train = torch.sqrt( torch.mean( (y_pred_train - y_train)**2 ) ).item()\n",
    "# validation rmse\n",
    "y_pred_val = model(x_val).detach()\n",
    "rmse_val = torch.sqrt( torch.mean( (y_pred_val - y_val)**2 ) ).item()\n",
    "# test rmse\n",
    "y_pred_test = model(x_test).detach()\n",
    "rmse_test = torch.sqrt( torch.mean( (y_pred_test - y_test)**2 ) ).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidian_dist(prediction, expected):\n",
    "    diff_squared = (prediction - expected) **2\n",
    "    diff_sum = diff_squared[:,0] + diff_squared[:,1]\n",
    "    dist = np.sqrt(diff_sum)\n",
    "\n",
    "    return dist\n",
    "\n",
    "train_dist = euclidian_dist(y_pred_train, y_train)\n",
    "val_dist = euclidian_dist(y_pred_val, y_val)\n",
    "test_dist = euclidian_dist(y_pred_test, y_test)\n",
    "\n",
    "# Next, getting the original coordinates across the manifold\n",
    "\n",
    "train_coord_dist = torch.cat((x_train, torch.unsqueeze(train_dist, dim = 1)), dim = 1)\n",
    "val_coord_dist = torch.cat((x_val, torch.unsqueeze(val_dist, dim = 1)), dim = 1)\n",
    "test_coord_dist = torch.cat((x_test, torch.unsqueeze(test_dist, dim = 1)), dim = 1)\n",
    "\n",
    "\n",
    "# Train\n",
    "unique_values_train = torch.unique(train_coord_dist[:, 2])\n",
    "groups_train = {}\n",
    "\n",
    "for value in unique_values_train:\n",
    "    indices = torch.nonzero(train_coord_dist[:, 2] == value).squeeze()\n",
    "    groups_train[value.item()] = train_coord_dist[indices]\n",
    "\n",
    "# Val\n",
    "unique_values_val = torch.unique(val_coord_dist[:, 2])\n",
    "groups_val = {}\n",
    "\n",
    "for value in unique_values_val:\n",
    "    indices = torch.nonzero(val_coord_dist[:, 2] == value).squeeze()\n",
    "    groups_val[value.item()] = val_coord_dist[indices]\n",
    "\n",
    "# Test\n",
    "unique_values_test = torch.unique(test_coord_dist[:, 2])\n",
    "groups_test = {}\n",
    "\n",
    "for value in unique_values_test:\n",
    "    indices = torch.nonzero(test_coord_dist[:, 2] == value).squeeze()\n",
    "    groups_test[value.item()] = test_coord_dist[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, want to get the average and standard deviation of the errors, like shift_from_average function\n",
    "\n",
    "# Training first\n",
    "\n",
    "train_keys = list(groups_train.keys())\n",
    "\n",
    "avgs_train = []\n",
    "stds_train = []\n",
    "\n",
    "for group in train_keys:\n",
    "\n",
    "    distances = np.array(groups_train[group])[:,3]\n",
    "    avg_dist = np.average(distances)\n",
    "    std = np.std(distances)\n",
    "\n",
    "    avgs_train +=[avg_dist]\n",
    "    stds_train += [std]\n",
    "\n",
    "\n",
    "val_keys = list(groups_val.keys())\n",
    "\n",
    "avgs_val = []\n",
    "stds_val = []\n",
    "\n",
    "for group in val_keys:\n",
    "\n",
    "    distances = np.array(groups_val[group])[:,3]\n",
    "    avg_dist = np.average(distances)\n",
    "    std = np.std(distances)\n",
    "\n",
    "    avgs_val +=[avg_dist]\n",
    "    stds_val += [std]\n",
    "\n",
    "\n",
    "test_keys = list(groups_test.keys())\n",
    "\n",
    "avgs_test = []\n",
    "stds_test = []\n",
    "\n",
    "for group in test_keys:\n",
    "\n",
    "    distances = np.array(groups_test[group])[:,3]\n",
    "    avg_dist = np.average(distances)\n",
    "    std = np.std(distances)\n",
    "\n",
    "    avgs_test +=[avg_dist]\n",
    "    stds_test += [std]\n",
    "\n",
    "# Turning data into dictionary \n",
    "\n",
    "error_fp_data = {'avgs_train' : avgs_train,\n",
    "                 'stds_train' :  stds_train,\n",
    "                 'avgs_val' :  avgs_val, \n",
    "                 'stds_val' :  stds_val,\n",
    "                 'avgs_test' :  avgs_test,\n",
    "                 'stds_test' :  stds_test}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv file\n",
    "with open('error_fp_data.csv', 'w', newline='') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    for key, value in data.items():\n",
    "        writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making error_for_datasets.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requires the same model code as the previous file. Copied here for continuity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import processed embedding file, nn_input, and nn_output\n",
    "df = pd.read_csv('no_avg_dataset.csv')\n",
    "nia = np.load(\"nn_input.npy\")\n",
    "noa = np.load(\"nn_output.npy\")\n",
    "seq_array = np.loadtxt(\"seq_arr.npy\", dtype = str).reshape(-1,1)\n",
    "\n",
    "# Get training, testing, and validation datasets, split by sequence\n",
    "test_val_splitter = GroupShuffleSplit(n_splits = 1, test_size = 0.6, random_state = 0)\n",
    "\n",
    "test_val_indices, train_indices = next(test_val_splitter.split(nia, noa, groups = seq_array))\n",
    "\n",
    "x_test_val, x_train = nia[test_val_indices], nia[train_indices]\n",
    "y_test_val, y_train = noa[test_val_indices], noa[train_indices]\n",
    "groups_test_val = seq_array[test_val_indices]\n",
    "\n",
    "val_test_splitter = GroupShuffleSplit(n_splits=1, test_size=0.5, random_state=0)\n",
    "val_indices, test_indices = next(val_test_splitter.split(x_test_val, y_test_val, groups=groups_test_val))\n",
    "\n",
    "x_val, x_test = x_test_val[val_indices], x_test_val[test_indices]\n",
    "y_val, y_test = y_test_val[val_indices], y_test_val[test_indices]\n",
    "\n",
    "test_seqs = seq_array[test_indices]\n",
    "val_seqs = seq_array[val_indices]\n",
    "\n",
    "#Turn them into tensors\n",
    "x_train = torch.Tensor(x_train)\n",
    "x_val = torch.Tensor(x_val)\n",
    "y_train = torch.Tensor(y_train)\n",
    "y_val = torch.Tensor(y_val)\n",
    "x_test = torch.Tensor(x_test)\n",
    "y_test = torch.Tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model to get the predicted values\n",
    "\n",
    "class MLPRegressor(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size=(100, ), activation=nn.ReLU()):\n",
    "        torch.manual_seed(0)  # control random effects\n",
    "\n",
    "        super(MLPRegressor, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        prev_layer_size = input_size\n",
    "        for layer_size in hidden_size:\n",
    "            layers.append(nn.Linear(prev_layer_size, layer_size))\n",
    "            layers.append(activation)\n",
    "            prev_layer_size = layer_size\n",
    "\n",
    "        layers.append(nn.Linear(prev_layer_size, output_size))\n",
    "        self.fc_layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = x.clone().detach().requires_grad_(True)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "def train_model(model, data, optimizer, n_epochs):\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    x_train, x_val, y_train, y_val = data\n",
    "\n",
    "    # do the training\n",
    "    pbar = tqdm.tqdm(np.arange(n_epochs))\n",
    "    for epoch in pbar:\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        yp = model(x_train)\n",
    "        # Compute Loss\n",
    "        loss = criterion(yp, y_train)\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Update progress bar\n",
    "        pbar.set_postfix_str(f'loss: {loss.item():.3e}')\n",
    "\n",
    "    return model\n",
    "\n",
    "# create a data structure to convert from str input to nn function\n",
    "activation_dict = {'tanh': nn.Tanh(),\n",
    "                   'relu': nn.ReLU(),\n",
    "                   'leaky_relu': nn.LeakyReLU(),\n",
    "                   'sigmoid': nn.Sigmoid(),\n",
    "                   'elu': nn.ELU()}\n",
    "\n",
    "def generate_model_and_optimizer(params):\n",
    "    # create a list of layer sizes from the start, end, and depth\n",
    "    raw_dims = np.linspace(params[\"layer_i\"], params[\"layer_f\"], params[\"num_layers\"])\n",
    "    hidden_size = tuple(raw_dims.round().astype(int))\n",
    "    # choose the activation function\n",
    "    activation = activation_dict[params['activation']]\n",
    "\n",
    "    # initialize the model\n",
    "    model = MLPRegressor(3, 2, hidden_size, activation)\n",
    "\n",
    "    # initialize the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "\n",
    "    # return both objects to calling function\n",
    "    return model, optimizer\n",
    "\n",
    "def mlp_fitness(params):\n",
    "    # the evaluation function cannot accept arguments -- we hard-code them here\n",
    "\n",
    "    data = (x_train, x_val, y_train, y_val)\n",
    "    try:\n",
    "        # train on training set\n",
    "        model, optimizer = generate_model_and_optimizer(params)\n",
    "        train_model(model, data, optimizer, params['n_epochs'])\n",
    "        # evaluate on held-out validation set\n",
    "        y_pred = model(x_val).detach()\n",
    "        rmse = torch.sqrt( torch.mean( (y_pred - y_val)**2 ) ).item()\n",
    "    except:\n",
    "        rmse = 1e12\n",
    "    rmse = min(rmse, 1e2)  # need to choose a threshold RMSE to handle errors\n",
    "    return np.log(rmse)    # log will be better behaved for very small numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1185/1185 [00:05<00:00, 215.59it/s, loss: 8.613e-01]\n"
     ]
    }
   ],
   "source": [
    "# Load model \n",
    "with open('best_parameters_absolute.json', 'r') as f:\n",
    "    best_parameters = json.load(f)\n",
    "\n",
    "model, optimizer = generate_model_and_optimizer(best_parameters)\n",
    "\n",
    "# Get predictedvalues\n",
    "data = (x_train, x_val, y_train, y_val)\n",
    "train_model(model, data, optimizer, best_parameters['n_epochs'])\n",
    "# train rmse\n",
    "y_pred_train = model(x_train).detach()\n",
    "rmse_train = torch.sqrt( torch.mean( (y_pred_train - y_train)**2 ) ).item()\n",
    "# validation rmse\n",
    "y_pred_val = model(x_val).detach()\n",
    "rmse_val = torch.sqrt( torch.mean( (y_pred_val - y_val)**2 ) ).item()\n",
    "# test rmse\n",
    "y_pred_test = model(x_test).detach()\n",
    "rmse_test = torch.sqrt( torch.mean( (y_pred_test - y_test)**2 ) ).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidian_dist(prediction, expected):\n",
    "    diff_squared = (prediction - expected) **2\n",
    "    diff_sum = diff_squared[:,0] + diff_squared[:,1]\n",
    "    dist = np.sqrt(diff_sum)\n",
    "\n",
    "    return dist\n",
    "\n",
    "# Get distances for three datasets\n",
    "train_dist = euclidian_dist(y_pred_train, y_train)\n",
    "val_dist = euclidian_dist(y_pred_val, y_val)\n",
    "test_dist = euclidian_dist(y_pred_test, y_test)\n",
    "\n",
    "# Combine coordinates and sitance data\n",
    "train_coord_dist = torch.cat((x_train, torch.unsqueeze(train_dist, dim = 1)), dim = 1)\n",
    "val_coord_dist = torch.cat((x_val, torch.unsqueeze(val_dist, dim = 1)), dim = 1)\n",
    "test_coord_dist = torch.cat((x_test, torch.unsqueeze(test_dist, dim = 1)), dim = 1)\n",
    "\n",
    "# Training set\n",
    "unique_values_train = torch.unique(train_coord_dist[:, 2])\n",
    "groups_train = {}\n",
    "\n",
    "# Sort by p\n",
    "for value in unique_values_train:\n",
    "    indices = torch.nonzero(train_coord_dist[:, 2] == value).squeeze()\n",
    "    groups_train[value.item()] = train_coord_dist[indices]\n",
    "\n",
    "# Validation set\n",
    "unique_values_val = torch.unique(val_coord_dist[:, 2])\n",
    "groups_val = {}\n",
    "\n",
    "# Sort by p\n",
    "for value in unique_values_val:\n",
    "    indices = torch.nonzero(val_coord_dist[:, 2] == value).squeeze()\n",
    "    groups_val[value.item()] = val_coord_dist[indices]\n",
    "\n",
    "# Test set\n",
    "unique_values_test = torch.unique(test_coord_dist[:, 2])\n",
    "groups_test = {}\n",
    "\n",
    "# Sort by p\n",
    "for value in unique_values_test:\n",
    "    indices = torch.nonzero(test_coord_dist[:, 2] == value).squeeze()\n",
    "    groups_test[value.item()] = test_coord_dist[indices]\n",
    "\n",
    "# Save to dictionary\n",
    "dist_dict = {'train_coord_dist' : train_coord_dist,\n",
    "             'val_coord_dist' : val_coord_dist, \n",
    "             'test_coord_dist' : test_coord_dist}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing file \n",
    "torch.save(dist_dict, 'error_for_datasets.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making norm_data.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data\n",
    "df = pd.read_csv('no_avg_dataset.csv')\n",
    "\n",
    "# Load model \n",
    "with open('best_parameters_absolute.json', 'r') as f:\n",
    "    best_parameters = json.load(f)\n",
    "\n",
    "model, optimizer = generate_model_and_optimizer(best_parameters)\n",
    "model_dict = torch.load('embedding_model_noavg_absolute.pth')\n",
    "\n",
    "#Loading pre-saved weights to the model\n",
    "model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get points on a grid across the manifold \n",
    "\n",
    "# Getting coordinates and p values from dataframe\n",
    "ps = np.unique(df[\"p\"])\n",
    "z_array = np.concatenate((np.asarray(df[\"Z0\"]).reshape(-1,1), np.asarray(df[\"Z1\"]).reshape(-1,1)), axis = 1)\n",
    "\n",
    "# Getting x and y bounds for mesh\n",
    "max_x = max(z_array[:,0])\n",
    "max_y = max(z_array[:,1])\n",
    "\n",
    "min_x = min(z_array[:,0])\n",
    "min_y = min(z_array[:,1])\n",
    "\n",
    "x_range = np.linspace(min_x, max_x, 30)\n",
    "y_range = np.linspace(min_y, max_y, 30)\n",
    "X,Y = np.meshgrid(x_range, y_range)\n",
    "\n",
    "mesh_point_set = []\n",
    "\n",
    "#Making an array of all the points\n",
    "for i, row in enumerate(X): \n",
    "    row_mesh = np.concatenate((row.reshape(-1,1), Y[i].reshape(-1,1)), axis = 1)\n",
    "    mesh_point_set.append(row_mesh)\n",
    "\n",
    "mesh_set = np.vstack(mesh_point_set)\n",
    "\n",
    "# Making alpha shape\n",
    "alpha_shape = alphashape(z_array, alpha = 1)\n",
    "\n",
    "# Collecting mesh points that are inside the alpha shape\n",
    "alpha_shape_mesh_points = []\n",
    "for point in mesh_set:\n",
    "    point_shape = Point(point)\n",
    "    if alpha_shape.contains(point_shape) == True:\n",
    "        alpha_shape_mesh_points.append(point)\n",
    "alpha_shape_mesh = np.vstack(alpha_shape_mesh_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the norm values for a selected p \n",
    "\n",
    "def get_plot_norm(p, model, alpha_shape_mesh):\n",
    "    at_p = p  #What p value are we looking at\n",
    "\n",
    "    zero_arr = np.full((alpha_shape_mesh.shape[0], 1), at_p)\n",
    "    grid_pts = np.hstack((alpha_shape_mesh, zero_arr))\n",
    "\n",
    "    row_and_norm = []\n",
    "\n",
    "    for i, row in enumerate(grid_pts):\n",
    "        point = torch.Tensor(row)\n",
    "        point.requires_grad_(True)\n",
    "\n",
    "        #Forward pass\n",
    "        output = model(point)\n",
    "        z0, z1 = output\n",
    "\n",
    "        z0.backward(torch.tensor(1.0, dtype = torch.float), retain_graph = True)\n",
    "\n",
    "        gradient_norm_z0 = torch.norm(point.grad)\n",
    "\n",
    "        # Zero out gradients for z0 for next iteration\n",
    "        point.grad.zero_()\n",
    "\n",
    "        # Backward pass to compute gradients for z1\n",
    "        z1.backward(torch.tensor(1.0, dtype=torch.float))\n",
    "\n",
    "        # Access the gradient for z1\n",
    "        gradient_norm_z1 = torch.norm(point.grad)\n",
    "\n",
    "        point.grad.zero_()\n",
    "\n",
    "        # Save values \n",
    "        x = point[0].detach().numpy()\n",
    "        y = point[1].detach().numpy()\n",
    "        norm_z0 = gradient_norm_z0.item()  \n",
    "        norm_z1 = gradient_norm_z1.item()  \n",
    "\n",
    "        row_and_norm.append(np.array((x, y, norm_z0, norm_z1)))\n",
    "\n",
    "        \n",
    "    row_and_norm_arr = np.vstack(row_and_norm)\n",
    "\n",
    "    dxdp = row_and_norm_arr[:,2].reshape(-1,1)\n",
    "    dydp = row_and_norm_arr[:,3].reshape(-1,1)\n",
    "\n",
    "    # Get magnitude of the norm \n",
    "    magnitude = np.sqrt(dxdp**2 + dydp**2)\n",
    "\n",
    "    return row_and_norm_arr, magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the norm data\n",
    "\n",
    "# Going through each p\n",
    "ps = np.unique(df[\"p\"])\n",
    "\n",
    "# Getting norm data for each p and saving to a list \n",
    "norm_data = []\n",
    "for p in ps:\n",
    "    row_and_norm_arr, magnitude = get_plot_norm(p, model, alpha_shape_mesh)\n",
    "    norm_row_data = {'p' : p,\n",
    "                     'row_and_norm_arr' : row_and_norm_arr,\n",
    "                     'magnitude' : magnitude}\n",
    "    norm_data += [norm_row_data]\n",
    "\n",
    "# Convert the arrays in the dictionary to lists for saving\n",
    "serial_norm_data = [{k: v.tolist() for k, v in d.items()} for d in norm_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write file\n",
    "with open('norm_data.json', 'w') as f:\n",
    "    json.dump(serial_norm_data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
